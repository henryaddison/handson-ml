{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 Training Deep Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henryaddison/anaconda3/envs/handson-ml/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/henryaddison/anaconda3/envs/handson-ml/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/henryaddison/anaconda3/envs/handson-ml/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/henryaddison/anaconda3/envs/handson-ml/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "# input data placeholder\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "# tells batch normalization whether to use current mini-batch's mean & s.d. (when training)\n",
    "# or use the whole training set's mean & s.d. (during use)\n",
    "# Start with it False\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "# first layer of hidden nodes densely connected to the input\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "# apply batch normalization to the hidden1 output, with a momentum term\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "# apply activation function (ELU in this case) to the batch normalized output of 1st hidden layer\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "# same again for second hidden layer based on final activated and normalized first hidden layer output\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden1, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "# finally the output layer\n",
    "# first up the logits raw output\n",
    "logits_pre_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_pre_bn, training=training, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pain to keep repeating the args to batch_normalization\n",
    "# partial from functools is your friend\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "\n",
    "# then bn lines can become e.g.\n",
    "bn1 = my_batch_norm_layer(hidden1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume have the usual init, optimizer, training_op, eval and savers\n",
    "\n",
    "# But also for this need to set training to True while actually training\n",
    "# AND there are some every operations that need to be run during each training step so that the moving averages update\n",
    "# there's an automatic UPDATE_OPS collection for that\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            # here's the deviation: we also eval extra_update_ops AND override training flag to True\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        # and here when testing (so actually using the network rather than training), no need to override training flag\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.validation.imgges, y: mnist.validation.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "        \n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f4b9d2ff7b8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# but now use it to get the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# then cap them (-ve and +ve)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "learning_rate=0.01\n",
    "\n",
    "# set a threshold for gradients\n",
    "threshold = 1.0\n",
    "\n",
    "# create an optimizer like normal\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "# but now use it to get the gradients\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "\n",
    "# then cap them (-ve and +ve)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "             for grad, var in grads_and_vars]\n",
    "\n",
    "# and use the capped ones to adjust params\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use import_meta_graph to load the operations of previous model into default graph\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "# Or if you have access to it, you could use the original source code to build the graph\n",
    "\n",
    "# then use get_tensor_by_name to get tensors of pretrained model\n",
    "# NB for tensors uses name of op with any name scope AND the index to outputs of op\n",
    "# hopefully it is well documented or else can write out the graph and explore in tensor board\n",
    "# OR the get_operations method\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "# or for ops use get_operation_by_name\n",
    "training_op = tf.get_default_graph().get_operationp(\"GradientDescent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When writing models, can make re-use easier by creating collections containin useful subsets of operations\n",
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important_ops\", op)\n",
    "    \n",
    "# then anyone else can just get the collection\n",
    "X, y, accuracy, training_op = tf.get_collection(\"my_important_ops\")\n",
    "\n",
    "# Then use the saver in a session to restore the model state as normal\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    # train the various bits you want with your own data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to import model params from other frameworks (e.g. Theano) but it gets more painful.\n",
    "Once you have the model recreated as a TF graph and the weights loaded from the other model (will require code from the original framework) than can use the makei Assign operator for each variable to initialize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose have original_w and original_b from other framework and have built the graph - so each have a hidden1 layer\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernal = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
    "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
    "init_kernal = assign_kernel.inputs[1]\n",
    "init_bias = assign_bias.inputs[1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n",
    "    # ... train the model on new task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will work to start from the pre-trained model then continue to learn all the parameters.\n",
    "\n",
    "Can speed up training more if can\n",
    "\n",
    "### Freeze lower layers\n",
    "So assume that the lower layers of the DNN are sufficiently trained (e.g. correctly working for edge detection say in an image classifier) and that actually only need to train the parameters of the higher layers more.\n",
    "\n",
    "In Tensorflow there are two ways to achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly set the list of variables that the optimizer can alter\n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|outputs\")\n",
    "training_op = optimizer.minimum(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or add a stop_gradient layer to the graph to ensure won't train below it\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(...)# to be reused frozen\n",
    "    hidden2 = tf.layers.dense(...)# to be reused frozen\n",
    "    # ensure layers up to here are frozen\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, ...) # reused layer but not frozen OR could be an entirely new layer on top\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching the frozen layers\n",
    "For the frozen lower layers and a given dataset, the output of these lower layers will be unchanged each run for each example. Can get a speed up therefore by injecting a cached version of the lower layer output during training rather than recomputing it from scratch each time. Obviously with new examples need to make sure don't use this cache!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_batches = mnist.train.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    # restore the previously trained model\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpy\")\n",
    "    \n",
    "    # compute once the output for the frozen layers\n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: mnist.train.images})\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # shuffle the training images\n",
    "        shuffled_idx = np.random.permutation(mnist.train.num_examples)\n",
    "        # create training batches (examples & labels) based on the cached output of frozen layers\n",
    "        # instead of raw training examples & labels\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(mnist.train.labels[shuffled_idx], n_batches)\n",
    "        # running training op for each batch but now injecting the cached output of frozen layers instead of X\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2: hidden2_batch, y:y_batch})\n",
    "    \n",
    "    # save the new model based on the frozen old layers but with some more training with new examples (and maybe a new topology)\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
