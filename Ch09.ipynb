{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval(feed_dict={f: 1})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1, dtype=int32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "m,n = housing.data.shape\n",
    "housing_data_with_bias = np.c_[np.ones((m,1)), housing.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(housing_data_with_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"X\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(\n",
    "    tf.matmul(\n",
    "        tf.matrix_inverse(\n",
    "            tf.matmul(XT, X)\n",
    "        ),\n",
    "    XT),\n",
    "y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    [theta_value] = sess.run([theta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.7185181e+01],\n",
       "       [ 4.3633747e-01],\n",
       "       [ 9.3952334e-03],\n",
       "       [-1.0711310e-01],\n",
       "       [ 6.4479220e-01],\n",
       "       [-4.0338000e-06],\n",
       "       [-3.7813708e-03],\n",
       "       [-4.2348403e-01],\n",
       "       [-4.3721911e-01]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': 'California housing dataset.\\n\\nThe original database is available from StatLib\\n\\n    http://lib.stat.cmu.edu/datasets/\\n\\nThe data contains 20,640 observations on 9 variables.\\n\\nThis dataset contains the average house value as target variable\\nand the following input variables (features): average income,\\nhousing average age, average rooms, average bedrooms, population,\\naverage occupation, latitude, and longitude in that order.\\n\\nReferences\\n----------\\n\\nPace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\nStatistics and Probability Letters, 33 (1997) 291-297.\\n\\n',\n",
       " 'data': array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
       "           37.88      , -122.23      ],\n",
       "        [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
       "           37.86      , -122.22      ],\n",
       "        [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
       "           37.85      , -122.24      ],\n",
       "        ...,\n",
       "        [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
       "           39.43      , -121.22      ],\n",
       "        [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
       "           39.43      , -121.32      ],\n",
       "        [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
       "           39.37      , -121.24      ]]),\n",
       " 'feature_names': ['MedInc',\n",
       "  'HouseAge',\n",
       "  'AveRooms',\n",
       "  'AveBedrms',\n",
       "  'Population',\n",
       "  'AveOccup',\n",
       "  'Latitude',\n",
       "  'Longitude'],\n",
       " 'target': array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894])}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "### Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaled_housing_data_with_bias = StandardScaler().fit_transform(housing_data_with_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_with_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 8.1150465\n",
      "Epoch 100 MSE = 4.8860354\n",
      "Epoch 200 MSE = 4.8396173\n",
      "Epoch 300 MSE = 4.8292665\n",
      "Epoch 400 MSE = 4.8224063\n",
      "Epoch 500 MSE = 4.817423\n",
      "Epoch 600 MSE = 4.8137693\n",
      "Epoch 700 MSE = 4.8110876\n",
      "Epoch 800 MSE = 4.8091063\n",
      "Epoch 900 MSE = 4.8076496\n",
      "Epoch 1000 MSE = 4.8065658\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    print(\"Epoch\", n_epochs, \"MSE =\", mse.eval())\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_with_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 8.353867\n",
      "Epoch 100 MSE = 5.0926166\n",
      "Epoch 200 MSE = 4.93796\n",
      "Epoch 300 MSE = 4.8966713\n",
      "Epoch 400 MSE = 4.8706183\n",
      "Epoch 500 MSE = 4.8520327\n",
      "Epoch 600 MSE = 4.8386106\n",
      "Epoch 700 MSE = 4.828902\n",
      "Epoch 800 MSE = 4.8218737\n",
      "Epoch 900 MSE = 4.816786\n",
      "Epoch 1000 MSE = 4.813096\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    print(\"Epoch\", n_epochs, \"MSE =\", mse.eval())\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "training_op = optimizer.minimize(mse)\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 8.449053\n",
      "Epoch 100 MSE = 5.0091963\n",
      "Epoch 200 MSE = 4.9036345\n",
      "Epoch 300 MSE = 4.875345\n",
      "Epoch 400 MSE = 4.8576202\n",
      "Epoch 500 MSE = 4.8446245\n",
      "Epoch 600 MSE = 4.8349066\n",
      "Epoch 700 MSE = 4.8275905\n",
      "Epoch 800 MSE = 4.822065\n",
      "Epoch 900 MSE = 4.8178706\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval(feed_dict={X: scaled_housing_data_with_bias, y: housing.target.reshape(-1,1)}))\n",
    "\n",
    "        summary_str = mse_summary.eval(feed_dict={X: scaled_housing_data_with_bias, y: housing.target.reshape(-1,1)})\n",
    "        step = epoch\n",
    "        file_writer.add_summary(summary_str, step)\n",
    "        sess.run(training_op, feed_dict={X: scaled_housing_data_with_bias, y: housing.target.reshape(-1,1)})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/Users/henryaddison/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    X_batch = housing_data_with_bias[batch_index*batch_size:batch_index*(batch_size+1), :]\n",
    "    y_batch = housing.target.reshape(-1,1)[batch_index*batch_size:batch_index*(batch_size+1), :]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "training_op = optimizer.minimize(mse)\n",
    "training_op = optimizer.minimize(mse)\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/Users/henryaddison/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/henryaddison/my_model_final.ckpt\n",
      "[[-0.80491114]\n",
      " [ 0.87952006]\n",
      " [ 0.15557787]\n",
      " [-0.30842516]\n",
      " [ 0.31857252]\n",
      " [ 0.00824518]\n",
      " [-0.04350524]\n",
      " [-0.5845639 ]\n",
      " [-0.5585413 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/Users/henryaddison/my_model_final.ckpt\")\n",
    "    print(theta.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "## 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset size\n",
    "m = 10_000\n",
    "\n",
    "data, labels = make_moons(n_samples = m, noise=0.15, random_state=42)\n",
    "labels = labels.reshape(-1,1)\n",
    "\n",
    "# number of features for a data point\n",
    "n = len(data[0])\n",
    "\n",
    "# add bias term\n",
    "data = np.c_[np.ones((m,1)), data]\n",
    "\n",
    "data_train = data[0:8_000, :]\n",
    "labels_train = labels[0:8_000, :]\n",
    "\n",
    "data_test = data[8_000:, :]\n",
    "labels_test = labels[8_000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continuing training from epoch 9901\n",
      "INFO:tensorflow:Restoring parameters from ./ch09-tf-log-reg/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(X, y, learning_rate=0.001):\n",
    "    n_inputs_including_bias = int(X.get_shape()[1])\n",
    "    with tf.name_scope(\"model\"):\n",
    "        theta = tf.Variable(tf.random_uniform([n_inputs_including_bias, 1], -1.0, 1.0), name=\"theta\")\n",
    "        p_hat = tf.sigmoid(tf.matmul(X, theta), name=\"p_hat\")\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        loss = tf.losses.log_loss(y, p_hat, scope=\"loss\")\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "        loss_summary = tf.summary.scalar('Loss', loss)\n",
    "\n",
    "    with tf.name_scope(\"init\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.name_scope(\"save\"):\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    return p_hat, loss, training_op, loss_summary, init, saver\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "epochs = 10000\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "checkpoint_path = \"./ch09-tf-log-reg/model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y, learning_rate=learning_rate)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, 'rb') as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"continuing training from epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "        start_epoch = 0\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        _, loss_val, summary_str = sess.run([training_op, loss, loss_summary], feed_dict={X: data_train, y: labels_train})\n",
    "        if epoch % 100 == 0:\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            print(\"Epoch\", epoch, \"loss:\", loss_val)\n",
    "            step = epoch\n",
    "            file_writer.add_summary(summary_str, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
